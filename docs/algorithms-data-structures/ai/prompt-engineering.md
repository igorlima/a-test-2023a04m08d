---
layout: default
title: Prompt Engineering
nav_order: 1
parent: Artificial Intelligence
grand_parent: Algorithms and Data Structures
permalink: /docs/algorithms-and-data-structures/ai/prompt-engineering
---

- [go back]({% link docs/algorithms-data-structures/ai/index.md %})

# LLM Prompt Engineering [^1]

While humans can interpret vague or incomplete information, LLMs require precise instructions to deliver optimal results.

It’s not simply about asking the model a question; it’s about phrasing, context, and understanding the model’s logic.

Natural Language Processing (NLP), by definition, is the field of artificial intelligence that focuses on the understanding and generation of human language by computers. It is employed in a wide range of applications, including voice assistants, machine translation, chatbots, and more. In other words, when we talk about NLP, we refer to computers' ability to understand and generate natural langugage.

Generative pre-trained transformer (GPT) - when we talk about prompt engineering, we are typically referring to the manipulation of the initial sequence to guide the GPT’s output. This involves carefully crafting the input to elicit a specific type of response or to steer the model’s behavior in a desired direction. The better the prompt, the more accurate and contextually relevant the output from GPT tends to be.

GPT stands for Generative Pre-trained Transformer. This architecture (transformer) is specifically designed for processing ang generating natural language, making it proficient in understanding and producing human-like text.

Transformer architectures are particulary effective in processing sequences of data, such as text, and are known for their ability to capture long-term dependencies in the data.

Other models:
- **GPT-3**: _A set of models capable of understanding and generating natural language._
- **DALL-E**: _An image generation model that can create images from textual descriptions. It can generate high-quality images in a wide variety of styles, sizes, and resolutions._
- **Whisper**: _A model that can convert audio input into text with high accuracy._
- **Embeddings**: _A set of models capable of converting text into numerical representations._
- **Moderation**: _A fine-tuned model capable of detecting if text may be sensitive or unsafe._
- **Codex**: _A set of models capable of understanding and generating code, including translating natural language into code._

GPT models process text in chunks called tokens. Tokens represent frequently occurring character sequences and they are different from words. It is all about building statistical relationships between words and contexts.

An API have the option to adjust certain parameters, including temperature, when integrating language models like GPT into their applications. Understanding and adjusting API parameters offers multiple advantages.

Here is a list of the most common parameters:
- Temperature
- Top-p
- Top-k
- Sequence length (max_tokens)
- Number of responses (n)
- Presence penalty (presence_penalty)
- Frequency penalty (frequency_penalty)
- Best of (best_of)

Perplexity applied to a text, could be used to detect if it was generated by a language model. If the perplexity is very low, it is likely that the text was generated by a language model. Some AI content detection tools, like [GPTZerp](https://gptzero.me) user this technique to detect if a text was generated by ChatGPT.

[Betterprompt](https://github.com/stjordanis/betterprompt) is a test suite for LLM prompts before pushing them to production. This open source tools is inspired by the same paper, "Demystifying Prompts in Language Models via Perplexity Estimation. Betterprompt helps in calculating the perplexity of a prompt.

The lower the perplexity, the better the prompt is.

![image](https://github.com/igorlima/unapologetic-snippets/assets/1886786/32aa53aa-876a-499b-96d8-d95a54c42854)

![image](https://github.com/igorlima/unapologetic-snippets/assets/1886786/a2ad0642-d2ea-479d-ab1c-79f2838b185a)

![image](https://github.com/igorlima/unapologetic-snippets/assets/1886786/ee7c67e7-9dc8-4f96-8802-233303d3bc1a)




## Hack the prompt [^1]

__*How can we lower the perplexity of a prompt?*__

What if we ask the model to create the prompt for us? This technique could be useful since the model is more familiar with the language it generates according to what we learned in the previous section.

Imagine you want to ask the model the following question:
- `Write a blog post about aging gracefully.`

We can use the following prompt to ask the model to create a prompt for us:
- `Write a list of prompts that I will use to write a blog post about aging gracefully.`

If you already have a list of prompts, you can ask the model to paraphrase them for you.
```
Paraphrase the following prompts:
1. What does aging gracefully mean to you personally?
2. Why is the topic of aging gracefully so important in today's society?
```

__Use double quotes__: if you want to emphasize certain words or phrases in your prompt, use double quotes. This could help the model understand the importance or specific focus of those words.

__Use single quotes when needed__: use single quotes to indicate a quote within a quote. This helps the model understand the context of the quote and generate a relevant response.

__Use text separators__: use text separators such as `"""` and `===` to separate different sections of your prompt. This helps the model understand the structure of your prompt and generate a relevant response.

## How can you write better prompts when you’re feeling lazy? [^2]

The main idea behind meta-prompting is: you are better at assessing the quality of a prompt than you are at writing one.

Make the model write a better prompt for you. Here’s the compressed meta-prompt:
```
Act as an expert Prompt Engineer.
I'll give you a messy prompt.
Reason step by step to improve it.
Write the final prompt as an elegant template with clear sections.
Use lists, placeholders, and examples.
##
Prompt:"""<Insert your prompt here, and yes please, use the triple quotes.>"""
```

Prompt Engineering is a fancy way to say _**“Write better and better instructions for AI until it does exactly what you want"**_.

The idea of meta-prompting is to use that very skill to your advantage. You make your model ask itself a better question than the one you’d ask yourself.

And just like with people, you get better answers if you ask better.

To build the quick fix we’ll use three [prompting techniques](https://towardsdatascience.com/how-to-write-expert-prompts-for-chatgpt-gpt-4-and-other-language-models-23133dc85550) to create an efficient meta-prompt:

- **Role Prompting**: _you give a role to your LLM, which indirectly specifies the context, objective, and other parameters like the style._
- **Chain-of-Thought prompting (CoT)**: _also known as “Reason step by step.” This is the most powerful sentence you can use when prompting an LLM. When LLMs “reason step by step,” they use tokens to “think” through stochastic predictions, which increases accuracy._
- **Placeholders**: _this is a way to both write and submit flexible prompts. Placeholders `<like_this>` allow you to play with different inputs and pick from a set of options._

----

[^1]: [LLM Prompt Engineering For Developers](https://leanpub.com/LLM-Prompt-Engineering-For-Developers)
[^2]: [How to Improve Any Prompt in Less Than 5 Minutes (Chat UI and Code)](https://towardsdatascience.com/how-to-improve-any-prompt-in-less-than-5-minutes-chat-ui-and-code-8a819e2fa2ba)
